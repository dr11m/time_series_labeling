{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc1b870",
   "metadata": {},
   "source": [
    "## *Since this is not part of the main labeling process, some dependencies need to be installed (including the kernel).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿ÑƒÑ‚ÑŒ Ðº ÐºÐ¾Ñ€Ð½ÐµÐ²Ð¾Ð¹ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ñ‹ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸\n",
    "from src.data_formats import TimeSeriesDataset, TimeSeries, TimeSeriesPoint, JSONAdapter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0c110",
   "metadata": {},
   "source": [
    "## **Here you need to load your dataset from your source in TimeSeriesDataset format.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get from any kind of source TimeSeriesDataset\n",
    "dataset: TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e536a7",
   "metadata": {},
   "source": [
    "## Loading data from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f45bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_from_json(input_path):\n",
    "    \"\"\"Load a TimeSeriesDataset from a JSON file\"\"\"\n",
    "    try:\n",
    "        adapter = JSONAdapter()\n",
    "        dataset = adapter.load_data(input_path)\n",
    "        print(f\"Dataset successfully loaded from {input_path}\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load dataset from JSON\n",
    "input_file = \"../data/csdeals_sales_dataset_.json\"\n",
    "dataset = load_dataset_from_json(input_file)\n",
    "\n",
    "if dataset:\n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"File: {input_file}\")\n",
    "    \n",
    "    # Show statistics\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"    - Total time series: {len(dataset)}\")\n",
    "    print(f\"    - Unlabeled series: {len(dataset.get_unlabeled_series())}\")\n",
    "    print(f\"    - Labeled series: {len(dataset.get_labeled_series())}\")\n",
    "    \n",
    "    # Show series examples\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nTime series examples:\")\n",
    "        for i, series in enumerate(dataset.series[:3]):\n",
    "            print(f\"  {i+1}. {series.name} - {series.length()} points, price: {series.metadata['min_price']:.2f}-{series.metadata['max_price']:.2f}\")\n",
    "    \n",
    "    # Now the dataset is ready for use in the labeling application\n",
    "    # or for further processing\n",
    "else:\n",
    "    print(\"Failed to load dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9b075",
   "metadata": {},
   "source": [
    "# Filtering process based on trimmed_mean_hours + number of sales over different time periods\n",
    "main idea:\n",
    "\n",
    "1) *calculate the usual trimmed_mean for intervals, removing extreme min/max values*\n",
    "2) *based on time periods, we establish limits:*\n",
    "\n",
    "{1 day - 2 sales, 3 days - 5 sales, 7 days - 10 sales, 1 month - 18 sales}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081593ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FilterResult:\n",
    "    \"\"\"Result of checking a single filter\"\"\"\n",
    "    passed: bool\n",
    "    value: Optional[float] = None\n",
    "    details: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "class TimeSeriesFilter:\n",
    "    def __init__(self, settings: Dict[str, Any], reference_date: Optional[datetime] = None):\n",
    "        self.settings = settings\n",
    "        self.reference_date = reference_date or datetime.now()\n",
    "    \n",
    "    def filter_dataset(self, dataset: TimeSeriesDataset) -> TimeSeriesDataset:\n",
    "        \"\"\"Filter the entire dataset\"\"\"\n",
    "        filtered_series = []\n",
    "        \n",
    "        for ts in dataset.series:\n",
    "            if self._should_keep_series(ts):\n",
    "                filtered_series.append(ts)\n",
    "        \n",
    "        return TimeSeriesDataset(\n",
    "            name=f\"{dataset.name}_filtered\",\n",
    "            description=f\"Filtered version of {dataset.description or dataset.name}\",\n",
    "            series=filtered_series,\n",
    "            metadata={\n",
    "                **(dataset.metadata or {}),\n",
    "                \"filter_settings\": self.settings,\n",
    "                \"original_count\": len(dataset.series),\n",
    "                \"filtered_count\": len(filtered_series)\n",
    "            },\n",
    "            created_at=dataset.created_at\n",
    "        )\n",
    "    \n",
    "    def _should_keep_series(self, ts: TimeSeries) -> bool:\n",
    "        \"\"\"Determine whether to keep the time series\"\"\"\n",
    "        # Initialize filter metadata\n",
    "        if ts.metadata is None:\n",
    "            ts.metadata = {}\n",
    "        ts.metadata[\"filter_results\"] = {}\n",
    "        \n",
    "        # Convert data to convenient format\n",
    "        df = self._prepare_dataframe(ts)\n",
    "        \n",
    "        # Check each filter\n",
    "        checks = [\n",
    "            (\"window_size\", self._check_window_size(df)),\n",
    "            (\"avg_trimmed_hours\", self._check_avg_trimmed_hours(df)),\n",
    "            (\"max_consecutive_empty_days\", self._check_consecutive_empty_days(df)),\n",
    "            (\"periods\", self._check_periods(df))\n",
    "        ]\n",
    "        \n",
    "        # Save results and determine overall result\n",
    "        all_passed = True\n",
    "        for check_name, result in checks:\n",
    "            ts.metadata[\"filter_results\"][check_name] = {\n",
    "                \"passed\": result.passed,\n",
    "                \"value\": result.value,\n",
    "                \"details\": result.details\n",
    "            }\n",
    "            all_passed = all_passed and result.passed\n",
    "        \n",
    "        ts.metadata[\"filter_results\"][\"overall_passed\"] = all_passed\n",
    "        return all_passed\n",
    "    \n",
    "    def _prepare_dataframe(self, ts: TimeSeries) -> pd.DataFrame:\n",
    "        \"\"\"Prepare DataFrame from TimeSeries\"\"\"\n",
    "        data = []\n",
    "        for point in ts.points:\n",
    "            # Convert timestamp to datetime if needed\n",
    "            if isinstance(point.timestamp, (int, float)):\n",
    "                dt = datetime.fromtimestamp(point.timestamp)\n",
    "            else:\n",
    "                dt = pd.to_datetime(point.timestamp)\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': dt,\n",
    "                'value': point.value\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        return df\n",
    "    \n",
    "    def _check_window_size(self, df: pd.DataFrame) -> FilterResult:\n",
    "        \"\"\"Check minimum window size\"\"\"\n",
    "        length = len(df)\n",
    "        required = self.settings['window_size']\n",
    "        \n",
    "        return FilterResult(\n",
    "            passed=length >= required,\n",
    "            value=length,\n",
    "            details={\"required\": required, \"actual\": length}\n",
    "        )\n",
    "    \n",
    "    def _check_avg_trimmed_hours(self, df: pd.DataFrame) -> FilterResult:\n",
    "        \"\"\"Check average time between sales (in hours)\"\"\"\n",
    "        if len(df) < 2:\n",
    "            return FilterResult(\n",
    "                passed=False,\n",
    "                value=None,\n",
    "                details={\"reason\": \"Not enough data points for interval calculation\"}\n",
    "            )\n",
    "        \n",
    "        # Calculate intervals between sales\n",
    "        intervals = df['timestamp'].diff().dropna()\n",
    "        intervals_hours = intervals.dt.total_seconds() / 3600\n",
    "        \n",
    "        # Remove extreme values (trimming)\n",
    "        trimmed_intervals = self._trim_outliers(intervals_hours.values)\n",
    "        avg_hours = np.mean(trimmed_intervals) if len(trimmed_intervals) > 0 else float('inf')\n",
    "        \n",
    "        max_allowed = self.settings['avg_trimmed_hours_max']\n",
    "        \n",
    "        return FilterResult(\n",
    "            passed=avg_hours <= max_allowed,\n",
    "            value=avg_hours,\n",
    "            details={\n",
    "                \"max_allowed\": max_allowed,\n",
    "                \"intervals_count\": len(intervals_hours),\n",
    "                \"trimmed_count\": len(trimmed_intervals)\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def _check_consecutive_empty_days(self, df: pd.DataFrame) -> FilterResult:\n",
    "        \"\"\"Check maximum number of consecutive empty days in the last N days\"\"\"\n",
    "        \n",
    "        period_days = self.settings.get('consecutive_empty_days_period', 14)\n",
    "        max_allowed = self.settings['max_consecutive_empty_days']\n",
    "        \n",
    "        # Define period for analysis (last N days from reference_date)\n",
    "        reference_date = self.reference_date.date()\n",
    "        start_date = reference_date - timedelta(days=period_days - 1)\n",
    "        \n",
    "        # Create full calendar for the period\n",
    "        all_dates = pd.date_range(start=start_date, end=reference_date, freq='D').date\n",
    "        \n",
    "        # Get dates with sales in the period\n",
    "        period_df = df[\n",
    "            (df['timestamp'].dt.date >= start_date) & \n",
    "            (df['timestamp'].dt.date <= reference_date)\n",
    "        ]\n",
    "        sales_dates = set(period_df['date'].unique()) if not period_df.empty else set()\n",
    "    \n",
    "        # Find maximum consecutive sequence of empty days\n",
    "        max_consecutive = 0\n",
    "        current_consecutive = 0\n",
    "        empty_periods = []\n",
    "        period_start = None\n",
    "        \n",
    "        for date in all_dates:\n",
    "            if date not in sales_dates:\n",
    "                # Empty day\n",
    "                if current_consecutive == 0:\n",
    "                    period_start = date\n",
    "                current_consecutive += 1\n",
    "                max_consecutive = max(max_consecutive, current_consecutive)\n",
    "            else:\n",
    "                # Day with sales - break the sequence\n",
    "                if current_consecutive > 0:\n",
    "                    empty_periods.append({\n",
    "                        \"start\": str(period_start),\n",
    "                        \"end\": str(date - timedelta(days=1)),\n",
    "                        \"days\": current_consecutive\n",
    "                    })\n",
    "                current_consecutive = 0\n",
    "        \n",
    "        # If period ends with empty days\n",
    "        if current_consecutive > 0:\n",
    "            empty_periods.append({\n",
    "                \"start\": str(period_start),\n",
    "                \"end\": str(reference_date),\n",
    "                \"days\": current_consecutive\n",
    "            })\n",
    "        \n",
    "        return FilterResult(\n",
    "            passed=max_consecutive <= max_allowed,\n",
    "            value=max_consecutive,\n",
    "            details={\n",
    "                \"max_allowed\": max_allowed,\n",
    "                \"period_start\": str(start_date),\n",
    "                \"period_end\": str(reference_date),\n",
    "                \"period_days\": period_days,\n",
    "                \"total_days_checked\": len(all_dates),\n",
    "                \"sales_days\": len(sales_dates),\n",
    "                \"empty_days\": len(all_dates) - len(sales_dates),\n",
    "                \"empty_periods\": empty_periods\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _check_periods(self, df: pd.DataFrame) -> FilterResult:\n",
    "        \"\"\"Check period requirements\"\"\"\n",
    "        if df.empty:\n",
    "            return FilterResult(\n",
    "                passed=False,\n",
    "                value=None,\n",
    "                details={\"reason\": \"No data available\"}\n",
    "            )\n",
    "        \n",
    "        # Use passed date or current date\n",
    "        reference_date = self.reference_date.date()\n",
    "        \n",
    "        period_results = {}\n",
    "        all_passed = True\n",
    "        \n",
    "        for period_days, min_sales in self.settings['periods'].items():\n",
    "            start_date = reference_date - timedelta(days=period_days - 1)\n",
    "            \n",
    "            # Count sales in the period\n",
    "            period_sales = df[\n",
    "                (df['timestamp'].dt.date >= start_date) & \n",
    "                (df['timestamp'].dt.date <= reference_date)\n",
    "            ]\n",
    "            actual_sales = len(period_sales)\n",
    "            \n",
    "            passed = actual_sales >= min_sales\n",
    "            all_passed = all_passed and passed\n",
    "            \n",
    "            period_results[f\"last_{period_days}_days\"] = {\n",
    "                \"required\": min_sales,\n",
    "                \"actual\": actual_sales,\n",
    "                \"passed\": passed,\n",
    "                \"period_start\": str(start_date),\n",
    "                \"period_end\": str(reference_date),\n",
    "                \"reference_date\": str(reference_date)\n",
    "            }\n",
    "        \n",
    "        return FilterResult(\n",
    "            passed=all_passed,\n",
    "            value=None,\n",
    "            details=period_results\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def _trim_outliers(self, values: np.ndarray, trim_percent: float = 0.1) -> np.ndarray:\n",
    "        \"\"\"Remove outliers from value array\"\"\"\n",
    "        if len(values) <= 2:\n",
    "            return values\n",
    "        \n",
    "        q_low = np.percentile(values, trim_percent * 100)\n",
    "        q_high = np.percentile(values, (1 - trim_percent) * 100)\n",
    "        \n",
    "        return values[(values >= q_low) & (values <= q_high)]\n",
    "\n",
    "\n",
    "# Function for convenient usage\n",
    "def apply_filter(dataset: TimeSeriesDataset, \n",
    "                filter_settings: Dict[str, Any], \n",
    "                reference_date: Optional[datetime] = None) -> TimeSeriesDataset:\n",
    "    \"\"\"\n",
    "    Apply filter to dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset: Original dataset\n",
    "        filter_settings: Filter settings\n",
    "        reference_date: Reference date for period checks (default - current)\n",
    "    \n",
    "    Returns:\n",
    "        Filtered dataset\n",
    "    \"\"\"\n",
    "    filter_instance = TimeSeriesFilter(filter_settings, reference_date)\n",
    "    return filter_instance.filter_dataset(dataset)\n",
    "\n",
    "\n",
    "\n",
    "# Function for analyzing filter results\n",
    "def analyze_filter_results(dataset: TimeSeriesDataset) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze filter results\"\"\"\n",
    "    total_series = len(dataset.series)\n",
    "    passed_series = sum(1 for ts in dataset.series \n",
    "                       if ts.metadata and ts.metadata.get(\"filter_results\", {}).get(\"overall_passed\", False))\n",
    "    \n",
    "    # Statistics for each filter\n",
    "    filter_stats = {}\n",
    "    filter_names = [\"window_size\", \"avg_trimmed_hours\", \"max_consecutive_empty_days\", \"periods\"]\n",
    "    \n",
    "    for filter_name in filter_names:\n",
    "        passed_count = sum(1 for ts in dataset.series \n",
    "                          if ts.metadata and \n",
    "                          ts.metadata.get(\"filter_results\", {}).get(filter_name, {}).get(\"passed\", False))\n",
    "        filter_stats[filter_name] = {\n",
    "            \"passed\": passed_count,\n",
    "            \"total\": total_series,\n",
    "            \"pass_rate\": passed_count / total_series if total_series > 0 else 0\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"total_series\": total_series,\n",
    "        \"passed_series\": passed_series,\n",
    "        \"overall_pass_rate\": passed_series / total_series if total_series > 0 else 0,\n",
    "        \"filter_stats\": filter_stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67129256",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SETTINGS = {\n",
    "    'window_size': 10,\n",
    "    'avg_trimmed_hours_max': 70,\n",
    "    'max_consecutive_empty_days': 5,  # No more than 5 consecutive gap days\n",
    "    'consecutive_empty_days_period': 14,  # Within the last 14 days\n",
    "    'periods': {\n",
    "        3: 2,\n",
    "        7: 5,\n",
    "        14: 9,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Apply filter\n",
    "filtered_dataset = apply_filter(dataset, FILTER_SETTINGS)\n",
    "\n",
    "\n",
    "# Analyze results\n",
    "results = analyze_filter_results(dataset)\n",
    "print(f\"Passed filtering: {results['passed_series']} out of {results['total_series']}\")\n",
    "print(f\"Overall pass rate: {results['overall_pass_rate']:.2%}\")\n",
    "\n",
    "\n",
    "# Detailed statistics for each filter\n",
    "for filter_name, stats in results['filter_stats'].items():\n",
    "    print(f\"{filter_name}: {stats['passed']}/{stats['total']} ({stats['pass_rate']:.2%})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0c5e4",
   "metadata": {},
   "source": [
    "## Visualization of accepted series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a8362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def show_accepted_series_analysis(dataset, max_examples=10, filter_settings=None):\n",
    "    \"\"\"Show analysis of accepted series with individual plots and timestamps, visualizing filter periods.\"\"\"\n",
    "    \n",
    "    print(f\"\\nAnalysis of accepted series (showing up to {max_examples} examples):\")\n",
    "    \n",
    "    if not dataset or len(dataset.series) == 0:\n",
    "        print(\"No accepted series for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Collect only accepted series\n",
    "    accepted_series = []\n",
    "    for series in dataset.series:\n",
    "        filter_results = series.metadata.get('filter_results', {})\n",
    "        if filter_results.get('overall_passed', False):\n",
    "            accepted_series.append(series)\n",
    "    \n",
    "    if not accepted_series:\n",
    "        print(\"No accepted series found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== Overall statistics ===\")\n",
    "    print(f\"Total accepted: {len(accepted_series)}\")\n",
    "    print(f\"Total in dataset: {len(dataset.series)}\")\n",
    "    print(f\"Acceptance rate: {len(accepted_series)/len(dataset.series)*100:.1f}%\")\n",
    "    \n",
    "    # Get filter settings\n",
    "    if filter_settings is None:\n",
    "        # Try to extract from dataset metadata\n",
    "        filter_settings = dataset.metadata.get('filter_settings', {})\n",
    "    \n",
    "    periods = filter_settings.get('periods', {14: 9})\n",
    "    max_period_days = max(periods.keys()) if periods else 14\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Sort by quality (fewer gaps = better)\n",
    "    def get_quality_score(series):\n",
    "        filter_results = series.metadata.get('filter_results', {})\n",
    "        consecutive_empty = filter_results.get('max_consecutive_empty_days', {}).get('value', 0)\n",
    "        avg_hours = filter_results.get('avg_trimmed_hours', {}).get('value', 999)\n",
    "        return consecutive_empty * 10 + avg_hours  # Penalty for gaps is higher\n",
    "    \n",
    "    sorted_accepted = sorted(accepted_series, key=get_quality_score)\n",
    "    \n",
    "    # Show best examples\n",
    "    examples_to_show = min(max_examples, len(sorted_accepted))\n",
    "    print(f\"\\n=== Showing {examples_to_show} best examples ===\")\n",
    "    \n",
    "    for i in range(examples_to_show):\n",
    "        series = sorted_accepted[i]\n",
    "        filter_results = series.metadata.get('filter_results', {})\n",
    "        \n",
    "        # Prepare data\n",
    "        timestamps = series.get_timestamps()\n",
    "        values = series.get_values()\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        dates_all = []\n",
    "        for ts in timestamps:\n",
    "            if isinstance(ts, (int, float)):\n",
    "                dates_all.append(datetime.fromtimestamp(ts))\n",
    "            else:\n",
    "                dates_all.append(pd.to_datetime(ts).to_pydatetime())\n",
    "        \n",
    "        # Filter data by maximum period\n",
    "        start_date = current_date - timedelta(days=max_period_days)\n",
    "        dates_filtered = []\n",
    "        values_filtered = []\n",
    "        \n",
    "        for dt, val in zip(dates_all, values):\n",
    "            if dt >= start_date:\n",
    "                dates_filtered.append(dt)\n",
    "                values_filtered.append(val)\n",
    "        \n",
    "        # If no data in period, take last points\n",
    "        if not dates_filtered:\n",
    "            dates_filtered = dates_all[-5:] if len(dates_all) >= 5 else dates_all\n",
    "            values_filtered = values[-5:] if len(values) >= 5 else values\n",
    "        \n",
    "        # Add current date as virtual point\n",
    "        dates_filtered.append(current_date)\n",
    "        values_filtered.append(values_filtered[-1] if values_filtered else 0)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Calculate intervals between sales in hours\n",
    "        diffs_hours = []\n",
    "        for j in range(1, len(dates_filtered)):\n",
    "            delta = (dates_filtered[j] - dates_filtered[j-1]).total_seconds() / 3600\n",
    "            diffs_hours.append(round(delta, 2))\n",
    "        \n",
    "        print(f\"Series {i+1} - Time intervals (hours): {diffs_hours}\")\n",
    "        \n",
    "        # Plot historical sales\n",
    "        plt.plot(dates_filtered[:-1], values_filtered[:-1], 'g-o', \n",
    "                markersize=6, linewidth=2, label='Historical sales')\n",
    "        \n",
    "        # Current date\n",
    "        plt.plot(dates_filtered[-1], values_filtered[-1], 'r*', \n",
    "                markersize=12, label='Current date')\n",
    "        \n",
    "        # Dashed line to current date\n",
    "        if len(dates_filtered) > 1:\n",
    "            plt.plot([dates_filtered[-2], dates_filtered[-1]], \n",
    "                    [values_filtered[-2], values_filtered[-1]], \n",
    "                    'r--', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        # Highlight periods with colored bands\n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral', 'lightpink']\n",
    "        for idx, (period_days, min_sales) in enumerate(sorted(periods.items())):\n",
    "            period_start = current_date - timedelta(days=period_days)\n",
    "            plt.axvspan(period_start, current_date, alpha=0.2, \n",
    "                       color=colors[idx % len(colors)], \n",
    "                       label=f'{period_days}d period (need {min_sales} sales)')\n",
    "        \n",
    "        # Plot formatting\n",
    "        plt.xlabel('Time', fontsize=12)\n",
    "        plt.ylabel('Price', fontsize=12)\n",
    "        plt.title(f'ACCEPTED SERIES #{i+1}: {series.name or series.id}\\n'\n",
    "                 f'Quality Score: {get_quality_score(series):.1f} (lower = better)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Information block with filter results\n",
    "        info_lines = []\n",
    "        info_lines.append(\"FILTER RESULTS:\")\n",
    "        \n",
    "        # Window size\n",
    "        window_result = filter_results.get('window_size', {})\n",
    "        info_lines.append(f\"â€¢ Window size: {window_result.get('value', 'N/A')} \"\n",
    "                         f\"({'âœ“' if window_result.get('passed') else 'âœ—'})\")\n",
    "        \n",
    "        # Average trimmed hours\n",
    "        avg_hours_result = filter_results.get('avg_trimmed_hours', {})\n",
    "        info_lines.append(f\"â€¢ Avg interval: {avg_hours_result.get('value', 0):.1f}h \"\n",
    "                         f\"({'âœ“' if avg_hours_result.get('passed') else 'âœ—'})\")\n",
    "        \n",
    "        # Consecutive empty days\n",
    "        empty_days_result = filter_results.get('max_consecutive_empty_days', {})\n",
    "        info_lines.append(f\"â€¢ Max gaps: {empty_days_result.get('value', 0)} days \"\n",
    "                         f\"({'âœ“' if empty_days_result.get('passed') else 'âœ—'})\")\n",
    "        \n",
    "        # Periods\n",
    "        periods_result = filter_results.get('periods', {})\n",
    "        info_lines.append(\"â€¢ Periods:\")\n",
    "        if periods_result.get('details'):\n",
    "            for period_key, period_data in periods_result['details'].items():\n",
    "                period_days = period_key.replace('last_', '').replace('_days', '')\n",
    "                info_lines.append(f\"   - {period_days}d: {period_data['actual']}/{period_data['required']} \"\n",
    "                                f\"({'âœ“' if period_data['passed'] else 'âœ—'})\")\n",
    "        \n",
    "        info_text = '\\n'.join(info_lines)\n",
    "        \n",
    "        plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, \n",
    "                verticalalignment='top', fontsize=9,\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   Series {i+1}: {series.name or series.id} - Quality Score: {get_quality_score(series):.1f}\")\n",
    "\n",
    "\n",
    "def show_filter_statistics(dataset, filter_settings):\n",
    "    \"\"\"Show detailed filter statistics\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED FILTER STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_series = len(dataset.series)\n",
    "    passed_series = sum(1 for s in dataset.series \n",
    "                       if s.metadata.get('filter_results', {}).get('overall_passed', False))\n",
    "    \n",
    "    print(f\"Overall: {passed_series}/{total_series} ({passed_series/total_series*100:.1f}%) passed\")\n",
    "    \n",
    "    # Analysis for each filter\n",
    "    filters = ['window_size', 'avg_trimmed_hours', 'max_consecutive_empty_days', 'periods']\n",
    "    \n",
    "    for filter_name in filters:\n",
    "        print(f\"\\nðŸ“Š {filter_name.upper()}:\")\n",
    "        \n",
    "        passed = 0\n",
    "        failed_values = []\n",
    "        passed_values = []\n",
    "        \n",
    "        for series in dataset.series:\n",
    "            result = series.metadata.get('filter_results', {}).get(filter_name, {})\n",
    "            if result.get('passed'):\n",
    "                passed += 1\n",
    "                if result.get('value') is not None:\n",
    "                    passed_values.append(result['value'])\n",
    "            else:\n",
    "                if result.get('value') is not None:\n",
    "                    failed_values.append(result['value'])\n",
    "        \n",
    "        failed = total_series - passed\n",
    "        print(f\"   Passed: {passed}/{total_series} ({passed/total_series*100:.1f}%)\")\n",
    "        print(f\"   Failed: {failed}/{total_series} ({failed/total_series*100:.1f}%)\")\n",
    "        \n",
    "        if failed_values:\n",
    "            print(f\"   Failed values - min: {min(failed_values):.2f}, \"\n",
    "                  f\"max: {max(failed_values):.2f}, \"\n",
    "                  f\"avg: {np.mean(failed_values):.2f}\")\n",
    "        \n",
    "        if passed_values:\n",
    "            print(f\"   Passed values - min: {min(passed_values):.2f}, \"\n",
    "                  f\"max: {max(passed_values):.2f}, \"\n",
    "                  f\"avg: {np.mean(passed_values):.2f}\")\n",
    "\n",
    "\n",
    "# Usage:\n",
    "show_filter_statistics(dataset, FILTER_SETTINGS)\n",
    "show_accepted_series_analysis(filtered_dataset, max_examples=10, filter_settings=FILTER_SETTINGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acb788",
   "metadata": {},
   "source": [
    "## Visualization of rejected series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af573d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def show_rejected_for_filter(dataset, filter_name, max_examples=10, filter_settings=None):\n",
    "    \"\"\"\n",
    "    Show rejected series for a specific filter, sorted by closeness to acceptance\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset with filter results\n",
    "        filter_name: Filter name ('window_size', 'avg_trimmed_hours', 'max_consecutive_empty_days', 'periods')\n",
    "        max_examples: Maximum number of examples to show\n",
    "        filter_settings: Filter settings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REJECTED SERIES ANALYSIS FOR FILTER: {filter_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if not dataset or len(dataset.series) == 0:\n",
    "        print(\"No series in dataset\")\n",
    "        return\n",
    "    \n",
    "    # Collect series rejected specifically by this filter\n",
    "    rejected_by_filter = []\n",
    "    \n",
    "    for series in dataset.series:\n",
    "        filter_results = series.metadata.get('filter_results', {})\n",
    "        \n",
    "        # Check that series is rejected and specifically by this filter\n",
    "        if filter_name in filter_results:\n",
    "            filter_result = filter_results[filter_name]\n",
    "            \n",
    "            if not filter_result.get('passed', True):\n",
    "                # Calculate distance to threshold for this filter\n",
    "                distance = calculate_filter_distance(filter_name, filter_result, filter_settings)\n",
    "                rejected_by_filter.append((series, distance, filter_result))\n",
    "    \n",
    "    if not rejected_by_filter:\n",
    "        print(f\"No series rejected specifically by filter '{filter_name}'\")\n",
    "        return\n",
    "    \n",
    "    # Sort by closeness to threshold (smaller distance = closer to acceptance)\n",
    "    rejected_by_filter.sort(key=lambda x: x[1])\n",
    "    \n",
    "    total_rejected_by_filter = len(rejected_by_filter)\n",
    "    print(f\"Total series rejected by '{filter_name}': {total_rejected_by_filter}\")\n",
    "    \n",
    "    # Distance statistics\n",
    "    distances = [x[1] for x in rejected_by_filter if x != np.inf]\n",
    "    if distances:\n",
    "        print(f\"Distance statistics:\")\n",
    "        print(f\"  - Closest to acceptance: {min(distances):.2f}\")\n",
    "        print(f\"  - Farthest from acceptance: {max(distances):.2f}\")\n",
    "        print(f\"  - Average distance: {np.mean(distances):.2f}\")\n",
    "    \n",
    "    # Show examples\n",
    "    examples_to_show = min(max_examples, total_rejected_by_filter)\n",
    "    print(f\"\\nShowing {examples_to_show} series closest to passing '{filter_name}':\")\n",
    "    \n",
    "    # Get settings for visualization\n",
    "    if filter_settings is None:\n",
    "        filter_settings = dataset.metadata.get('filter_settings', {})\n",
    "    \n",
    "    periods = filter_settings.get('periods', {14: 9})\n",
    "    max_period_days = max(periods.keys()) if periods else 14\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    for i in range(examples_to_show):\n",
    "        series, distance, filter_result = rejected_by_filter[i]\n",
    "        \n",
    "        # Prepare data for plot\n",
    "        timestamps = series.get_timestamps()\n",
    "        values = series.get_values()\n",
    "        \n",
    "        # Convert timestamp\n",
    "        dates_all = []\n",
    "        for ts in timestamps:\n",
    "            if isinstance(ts, (int, float)):\n",
    "                dates_all.append(datetime.fromtimestamp(ts))\n",
    "            else:\n",
    "                dates_all.append(pd.to_datetime(ts).to_pydatetime())\n",
    "        \n",
    "        # Filter by maximum period\n",
    "        start_date = current_date - timedelta(days=max_period_days)\n",
    "        dates_filtered = []\n",
    "        values_filtered = []\n",
    "        \n",
    "        for dt, val in zip(dates_all, values):\n",
    "            if dt >= start_date:\n",
    "                dates_filtered.append(dt)\n",
    "                values_filtered.append(val)\n",
    "        \n",
    "        # If no data in period, take the last ones\n",
    "        if not dates_filtered:\n",
    "            dates_filtered = dates_all[-10:] if len(dates_all) >= 10 else dates_all\n",
    "            values_filtered = values[-10:] if len(values) >= 10 else values\n",
    "        \n",
    "        # Add current date\n",
    "        dates_filtered.append(current_date)\n",
    "        values_filtered.append(values_filtered[-1] if values_filtered else 0)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Calculate intervals between sales\n",
    "        diffs_hours = []\n",
    "        for j in range(1, len(dates_filtered)):\n",
    "            delta = (dates_filtered[j] - dates_filtered[j-1]).total_seconds() / 3600\n",
    "            diffs_hours.append(round(delta, 2))\n",
    "        \n",
    "        print(f\"\\nSeries {i+1} intervals (hours): {diffs_hours}\")\n",
    "        \n",
    "        # Sales plot\n",
    "        plt.plot(dates_filtered[:-1], values_filtered[:-1], 'orange', \n",
    "                marker='o', markersize=6, linewidth=2, label='Historical sales')\n",
    "        \n",
    "        # Current date\n",
    "        plt.plot(dates_filtered[-1], values_filtered[-1], 'r*', \n",
    "                markersize=12, label='Current date')\n",
    "        \n",
    "        # Dashed line to current date  \n",
    "        if len(dates_filtered) > 1:\n",
    "            plt.plot([dates_filtered[-2], dates_filtered[-1]], \n",
    "                    [values_filtered[-2], values_filtered[-1]], \n",
    "                    'r--', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        # Highlight periods with orange shades\n",
    "        colors = ['moccasin', 'peachpuff', 'navajowhite', 'papayawhip', 'bisque']\n",
    "        for idx, (period_days, min_sales) in enumerate(sorted(periods.items())):\n",
    "            period_start = current_date - timedelta(days=period_days)\n",
    "            plt.axvspan(period_start, current_date, alpha=0.2, \n",
    "                       color=colors[idx % len(colors)], \n",
    "                       label=f'{period_days}d period (need {min_sales} sales)')\n",
    "        \n",
    "        # Plot formatting\n",
    "        plt.xlabel('Time', fontsize=12)\n",
    "        plt.ylabel('Price', fontsize=12)\n",
    "        plt.title(f'REJECTED BY {filter_name.upper()} #{i+1}: {series.name or series.id}\\n'\n",
    "                 f'Distance to passing: {distance:.2f} (closer to 0 = closer to acceptance)', \n",
    "                 fontsize=14, fontweight='bold', color='darkorange')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Detailed filter information\n",
    "        info_lines = create_filter_info(filter_name, filter_result, filter_settings, distance)\n",
    "        info_text = '\\n'.join(info_lines)\n",
    "        \n",
    "        plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, \n",
    "                verticalalignment='top', fontsize=10,\n",
    "                bbox=dict(boxstyle='round,pad=0.5', facecolor='moccasin', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"   #{i+1}: {series.name or series.id} - Distance: {distance:.2f}\")\n",
    "\n",
    "\n",
    "def calculate_filter_distance(filter_name, filter_result, filter_settings):\n",
    "    \"\"\"Calculate distance from current value to acceptance threshold\"\"\"\n",
    "    \n",
    "    actual = filter_result.get('value')\n",
    "    if actual is None:\n",
    "        return np.inf\n",
    "    \n",
    "    if filter_name == 'window_size':\n",
    "        threshold = filter_settings.get('window_size', 10)\n",
    "        return max(0, threshold - actual)  # Need more data\n",
    "    \n",
    "    elif filter_name == 'avg_trimmed_hours':\n",
    "        threshold = filter_settings.get('avg_trimmed_hours_max', 700)\n",
    "        return max(0, actual - threshold)  # Need fewer hours\n",
    "    \n",
    "    elif filter_name == 'max_consecutive_empty_days':\n",
    "        max_consec = filter_settings.get('max_consecutive_empty_days', 5)\n",
    "        if isinstance(max_consec, dict):\n",
    "            threshold = max_consec.get('max_gaps', 5)\n",
    "        else:\n",
    "            threshold = max_consec\n",
    "        return max(0, actual - threshold)  # Need fewer gaps\n",
    "    \n",
    "    elif filter_name == 'periods':\n",
    "        # For periods take minimum missing number of sales\n",
    "        details = filter_result.get('details', {})\n",
    "        min_shortage = np.inf\n",
    "        \n",
    "        for period_key, period_data in details.items():\n",
    "            if not period_data.get('passed', True):\n",
    "                required = period_data.get('required', 0)\n",
    "                actual_sales = period_data.get('actual', 0)\n",
    "                shortage = required - actual_sales\n",
    "                min_shortage = min(min_shortage, shortage)\n",
    "        \n",
    "        return min_shortage if min_shortage != np.inf else 0\n",
    "    \n",
    "    return np.inf\n",
    "\n",
    "\n",
    "def create_filter_info(filter_name, filter_result, filter_settings, distance):\n",
    "    \"\"\"Create information block for specific filter\"\"\"\n",
    "    \n",
    "    info_lines = [f\"ðŸ” FILTER: {filter_name.upper()}\"]\n",
    "    info_lines.append(f\"Distance to passing: {distance:.2f}\")\n",
    "    info_lines.append(\"-\" * 25)\n",
    "    \n",
    "    if filter_name == 'window_size':\n",
    "        actual = filter_result.get('value', 0)\n",
    "        required = filter_settings.get('window_size', 10)\n",
    "        info_lines.append(f\"Actual length: {actual}\")\n",
    "        info_lines.append(f\"Required: {required}\")\n",
    "        info_lines.append(f\"Need {required - actual} more points\")\n",
    "    \n",
    "    elif filter_name == 'avg_trimmed_hours':\n",
    "        actual = filter_result.get('value', 0)\n",
    "        max_allowed = filter_settings.get('avg_trimmed_hours_max', 700)\n",
    "        info_lines.append(f\"Avg interval: {actual:.1f} hours\")\n",
    "        info_lines.append(f\"Max allowed: {max_allowed} hours\")\n",
    "        info_lines.append(f\"Exceeds by: {actual - max_allowed:.1f} hours\")\n",
    "    \n",
    "    elif filter_name == 'max_consecutive_empty_days':\n",
    "        actual = filter_result.get('value', 0)\n",
    "        max_consec = filter_settings.get('max_consecutive_empty_days', 5)\n",
    "        if isinstance(max_consec, dict):\n",
    "            max_allowed = max_consec.get('max_gaps', 5)\n",
    "            period = max_consec.get('period', 14)\n",
    "            info_lines.append(f\"Max gaps in {period}d: {actual}\")\n",
    "        else:\n",
    "            max_allowed = max_consec\n",
    "            info_lines.append(f\"Max consecutive gaps: {actual}\")\n",
    "        info_lines.append(f\"Max allowed: {max_allowed}\")\n",
    "        info_lines.append(f\"Exceeds by: {actual - max_allowed} days\")\n",
    "    \n",
    "    elif filter_name == 'periods':\n",
    "        info_lines.append(\"Period requirements:\")\n",
    "        details = filter_result.get('details', {})\n",
    "        for period_key, period_data in details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of specific filter\n",
    "#show_rejected_for_filter(dataset, 'max_consecutive_empty_days', max_examples=10, filter_settings=FILTER_SETTINGS)\n",
    "\n",
    "\n",
    "# Or for periods\n",
    "#show_rejected_for_filter(dataset, 'periods', max_examples=10, filter_settings=FILTER_SETTINGS) \n",
    "\n",
    "\n",
    "# Analysis of all filters at once\n",
    "analyze_all_filters(dataset, FILTER_SETTINGS, max_per_filter=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bd412",
   "metadata": {},
   "source": [
    "# The filtering process that leads to FILTERING_SETTINGS - the final filtration based on various statistical methods (cv, outliers, max_gaps, max_range vs mean, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_to_json(dataset, output_path):\n",
    "    \"\"\"Save TimeSeriesDataset to a JSON file\"\"\"\n",
    "    try:\n",
    "        adapter = JSONAdapter()\n",
    "        adapter.save_data(dataset, output_path)\n",
    "        print(f\"Dataset successfully saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving dataset: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Save dataset to JSON\n",
    "if dataset:\n",
    "    output_file = \"../data/dataset.json\"\n",
    "    #success = save_dataset_to_json(dataset, output_file)\n",
    "\n",
    "\n",
    "    print(f\"\\nDataset ready for use in the annotation application!\")\n",
    "    print(f\"File: {output_file}\")\n",
    "    \n",
    "    # Display statistics\n",
    "    print(f\"\\nDataset statistics:\")\n",
    "    print(f\"  - Total time series: {len(dataset)}\")\n",
    "    print(f\"  - Unlabeled series: {len(dataset.get_unlabeled_series())}\")\n",
    "    print(f\"  - Labeled series: {len(dataset.get_labeled_series())}\")\n",
    "    \n",
    "    # Display sample series\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\nSample time series:\")\n",
    "        for i, series in enumerate(dataset.series[:3]):\n",
    "            print(f\"  {i+1}. {series.name} - {series.length()} points, price: {series.metadata['min_price']:.2f}-{series.metadata['max_price']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6c725",
   "metadata": {},
   "source": [
    "Filter Dataset by Series Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda8088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional function to filter by series length\n",
    "def filter_dataset_by_length(dataset, min_length=8, max_length=None):\n",
    "    \"\"\"Filter dataset by time series length\"\"\"\n",
    "    filtered_dataset = dataset.filter_by_length(min_length, max_length)\n",
    "    \n",
    "    print(f\"Filtering by series length:\")\n",
    "    print(f\"  - Minimum length: {min_length}\")\n",
    "    print(f\"  - Maximum length: {max_length or 'unlimited'}\")\n",
    "    print(f\"  - Original series count: {len(dataset)}\")\n",
    "    print(f\"  - After filtering: {len(filtered_dataset)}\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "\n",
    "filtered_dataset_by_length = filter_dataset_by_length(dataset, min_length=10, max_length=9999)\n",
    "\n",
    "\n",
    "# Save filtered dataset\n",
    "if len(filtered_dataset_by_length) > 0:\n",
    "    filtered_output_file = \"../data/dataset_filtered_by_length.json\"\n",
    "    save_dataset_to_json(filtered_dataset_by_length, filtered_output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094db680",
   "metadata": {},
   "source": [
    "Remove Duplicate Timestamps from Series\n",
    "This is very specific to my dataset, as the minimum sales interval = 1 day (with multiple sales possible in a single day), but in my case this only distorted the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc884d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_timestamps(series):\n",
    "    \"\"\"Remove duplicate timestamps from a single series\"\"\"\n",
    "    if not series.points:\n",
    "        return series\n",
    "    \n",
    "    # Create dictionary for unique timestamps\n",
    "    unique_points = {}\n",
    "    \n",
    "    for point in series.points:\n",
    "        timestamp = point.timestamp\n",
    "        \n",
    "        # If timestamp exists, keep the point with higher value (or first)\n",
    "        if timestamp not in unique_points:\n",
    "            unique_points[timestamp] = point\n",
    "        else:\n",
    "            # Strategy: keep point with higher value\n",
    "            if point.value > unique_points[timestamp].value:\n",
    "                unique_points[timestamp] = point\n",
    "    \n",
    "    # Sort points by time\n",
    "    sorted_points = sorted(unique_points.values(), key=lambda x: x.timestamp)\n",
    "    \n",
    "    # Create new series without duplicates\n",
    "    cleaned_series = TimeSeries(\n",
    "        id=series.id,\n",
    "        name=series.name,\n",
    "        points=sorted_points,\n",
    "        metadata={\n",
    "            **series.metadata,\n",
    "            'duplicates_removed': True,\n",
    "            'original_length': len(series.points),\n",
    "            'cleaned_length': len(sorted_points),\n",
    "            'duplicates_count': len(series.points) - len(sorted_points)\n",
    "        },\n",
    "        labeled_values=series.labeled_values\n",
    "    )\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "\n",
    "def filter_dataset_remove_duplicates(dataset):\n",
    "    \"\"\"Filter dataset by removing duplicate timestamps\"\"\"\n",
    "    print(\"Removing duplicate timestamps...\")\n",
    "    \n",
    "    original_count = len(dataset)\n",
    "    total_duplicates = 0\n",
    "    cleaned_series = []\n",
    "    \n",
    "    for i, series in enumerate(dataset.series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing series {i+1}/{original_count}\")\n",
    "        \n",
    "        # Remove duplicates for current series\n",
    "        cleaned_series_item = remove_duplicate_timestamps(series)\n",
    "        \n",
    "        # Count removed duplicates\n",
    "        duplicates_removed = cleaned_series_item.metadata.get('duplicates_count', 0)\n",
    "        total_duplicates += duplicates_removed\n",
    "        \n",
    "        cleaned_series.append(cleaned_series_item)\n",
    "    \n",
    "    # Create new dataset\n",
    "    cleaned_dataset = TimeSeriesDataset(\n",
    "        name=f\"{dataset.name} (Duplicates Removed)\",\n",
    "        description=f\"Dataset with removed duplicate timestamps. Created {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        series=cleaned_series,\n",
    "        metadata={\n",
    "            **dataset.metadata,\n",
    "            'duplicates_removed': True,\n",
    "            'total_duplicates_removed': total_duplicates,\n",
    "            'original_series_count': original_count\n",
    "        },\n",
    "        created_at=datetime.now()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDuplicate removal completed:\")\n",
    "    print(f\"  - Original series count: {original_count}\")\n",
    "    print(f\"  - Total duplicates removed: {total_duplicates}\")\n",
    "    print(f\"  - Remaining series: {len(cleaned_dataset)}\")\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "# Apply duplicate filtering to existing dataset\n",
    "if dataset:\n",
    "    print(\"Applying duplicate filtering...\")\n",
    "    filtered_dataset_by_duplicates = filter_dataset_remove_duplicates(filtered_dataset_by_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536e02c",
   "metadata": {},
   "source": [
    "## Calculate Sale Time Statistics and Filter by Sale Time (main logic of filtering process)\n",
    "This cell defines functions to calculate sale time statistics (including trimmed averages, variation metrics, and outlier detection) for series and to filter the dataset based on these statistics using configurable settings. It applies the filter with specified parameters, updates the dataset, and prints filtering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sale_time_statistics(series, window_size=10):\n",
    "    \"\"\"Calculate sale time statistics for a time series including current date\"\"\"\n",
    "    if series.length() < window_size:\n",
    "        return None\n",
    "    \n",
    "    # Get last N points\n",
    "    recent_points = series.points[-window_size:]\n",
    "    \n",
    "    # Sort by time\n",
    "    sorted_points = sorted(recent_points, key=lambda x: x.timestamp)\n",
    "    \n",
    "    # Add current date as virtual point for last interval\n",
    "    current_timestamp = datetime.now().timestamp()\n",
    "    \n",
    "    # Create temporary list with current date\n",
    "    points_with_current = sorted_points + [type('Point', (), {'timestamp': current_timestamp})()]\n",
    "    \n",
    "    # Calculate intervals between sales in seconds (including to current date)\n",
    "    time_intervals = []\n",
    "    for i in range(1, len(points_with_current)):\n",
    "        interval = points_with_current[i].timestamp - points_with_current[i-1].timestamp\n",
    "        time_intervals.append(interval)\n",
    "    \n",
    "    \n",
    "    # Save interval to current date separately\n",
    "    current_interval = time_intervals[-1]  # Last interval - to current date\n",
    "    \n",
    "    # Check if interval to current date is minimum or maximum\n",
    "    is_current_min = current_interval == min(time_intervals)\n",
    "    is_current_max = current_interval == max(time_intervals)\n",
    "    \n",
    "    # Remove outliers (maximum and minimum time)\n",
    "    original_intervals = time_intervals.copy()\n",
    "    time_intervals.sort()\n",
    "    \n",
    "    # If interval to current date is min or max, do NOT remove it\n",
    "    if is_current_min and is_current_max:\n",
    "        # Interval to current date is both min and max (all intervals identical)\n",
    "        trimmed_intervals = time_intervals\n",
    "    elif is_current_min:\n",
    "        # Interval to current date is minimum, remove only maximum\n",
    "        trimmed_intervals = time_intervals[:-1]  # Remove only max\n",
    "    elif is_current_max:\n",
    "        # Interval to current date is maximum, remove only minimum\n",
    "        trimmed_intervals = time_intervals[1:]  # Remove only min\n",
    "    else:\n",
    "        # Interval to current date is neither min nor max, remove both outliers\n",
    "        trimmed_intervals = time_intervals[1:-1]  # Remove min and max\n",
    "    \n",
    "    # Convert to hours\n",
    "    intervals_hours = [interval / 3600 for interval in trimmed_intervals]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_interval_hours = sum(intervals_hours) / len(intervals_hours)\n",
    "    min_interval_hours = min(intervals_hours)\n",
    "    max_interval_hours = max(intervals_hours)\n",
    "    \n",
    "    # Methods to detect inadequate distribution:\n",
    "    \n",
    "    # 1. Coefficient of variation (CV) - standard deviation / mean\n",
    "    mean_val = avg_interval_hours\n",
    "    variance = sum((x - mean_val) ** 2 for x in intervals_hours) / len(intervals_hours)\n",
    "    std_dev = variance ** 0.5\n",
    "    coefficient_of_variation = std_dev / mean_val if mean_val > 0 else float('inf')\n",
    "    \n",
    "    # 2. Range relative to mean\n",
    "    range_ratio = (max_interval_hours - min_interval_hours) / mean_val if mean_val > 0 else float('inf')\n",
    "    \n",
    "    # 3. Check for clustering (grouping of close values)\n",
    "    sorted_intervals = sorted(intervals_hours)\n",
    "    gaps = [sorted_intervals[i+1] - sorted_intervals[i] for i in range(len(sorted_intervals)-1)]\n",
    "    avg_gap = sum(gaps) / len(gaps) if gaps else 0\n",
    "    max_gap = max(gaps) if gaps else 0\n",
    "    gap_ratio = max_gap / avg_gap if avg_gap > 0 else float('inf')\n",
    "    \n",
    "    # 4. Check for outliers (more than 2 standard deviations from mean)\n",
    "    outliers = [x for x in intervals_hours if abs(x - mean_val) > 2 * std_dev]\n",
    "    outlier_ratio = len(outliers) / len(intervals_hours)\n",
    "    \n",
    "    # Additional info about current interval\n",
    "    current_interval_hours = current_interval / 3600\n",
    "    \n",
    "    # Check if interval to current date is an outlier\n",
    "    is_current_interval_outlier = abs(current_interval_hours - mean_val) > 2 * std_dev\n",
    "    \n",
    "    return {\n",
    "        'avg_interval_hours': avg_interval_hours,\n",
    "        'min_interval_hours': min_interval_hours,\n",
    "        'max_interval_hours': max_interval_hours,\n",
    "        'coefficient_of_variation': coefficient_of_variation,\n",
    "        'range_ratio': range_ratio,\n",
    "        'gap_ratio': gap_ratio,\n",
    "        'outlier_ratio': outlier_ratio,\n",
    "        'total_intervals': len(intervals_hours),\n",
    "        'std_dev': std_dev,\n",
    "        'intervals_hours': intervals_hours,  # Save for analysis\n",
    "        'current_interval_hours': current_interval_hours,  # Interval to current date\n",
    "        'is_current_interval_outlier': is_current_interval_outlier,  # Is it an outlier\n",
    "        'is_current_min': is_current_min,  # Is interval to current date minimal\n",
    "        'is_current_max': is_current_max,  # Is interval to current date maximal\n",
    "        'analysis_includes_current': True,  # Flag that analysis includes current date\n",
    "        'trimming_info': f\"Outliers removed: {len(original_intervals) - len(trimmed_intervals)} (current interval {'preserved' if (is_current_min or is_current_max) else 'was not min/max'})\"\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "def filter_dataset_by_sale_time(dataset, \n",
    "                               window_size=10, \n",
    "                               avg_trimmed_hours_max=168,\n",
    "                               max_cv=0.8,           # Maximum coefficient of variation\n",
    "                               max_range_ratio=2.0,  # Maximum range/mean\n",
    "                               max_gap_ratio=5.0,    # Maximum gap\n",
    "                               max_outlier_ratio=0.3): # Maximum outlier ratio\n",
    "    \"\"\"\n",
    "    Filter dataset by sale time\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset: TimeSeriesDataset for filtering\n",
    "    - window_size: number of last sales for analysis (default 10)\n",
    "    - avg_trimmed_hours_max: maximum mean time between sales in hours (trimmed mean)\n",
    "    - max_cv: maximum coefficient of variation (standard deviation / mean)\n",
    "    - max_range_ratio: maximum range relative to mean ((max-min)/mean)\n",
    "    - max_gap_ratio: maximum gap between neighboring values\n",
    "    - max_outlier_ratio: maximum ratio of outliers (values > 2*std_dev from mean)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Filtering dataset by sale time...\")\n",
    "    print(f\"  - Analysis window: last {window_size} sales\")\n",
    "    print(f\"  - Max trimmed mean time: {avg_trimmed_hours_max} hours\")\n",
    "    print(f\"  - Max coefficient of variation: {max_cv}\")\n",
    "    print(f\"  - Max range/mean: {max_range_ratio}\")\n",
    "    print(f\"  - Max gap: {max_gap_ratio}\")\n",
    "    print(f\"  - Max outlier ratio: {max_outlier_ratio}\")\n",
    "    \n",
    "    original_count = len(dataset)\n",
    "    filtered_series = []\n",
    "    skipped_count = 0\n",
    "    skipped_time_count = 0\n",
    "    skipped_cv_count = 0\n",
    "    skipped_range_count = 0\n",
    "    skipped_gap_count = 0\n",
    "    skipped_outlier_count = 0\n",
    "    \n",
    "    # Collect info about rejected series for analysis\n",
    "    rejected_series = {\n",
    "        'cv': [],\n",
    "        'range_ratio': [],\n",
    "        'gap_ratio': [],\n",
    "        'outlier_ratio': []\n",
    "    }\n",
    "    \n",
    "    for i, series in enumerate(dataset.series):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing series {i+1}/{original_count}\")\n",
    "        \n",
    "        # Calculate sale time statistics\n",
    "        time_stats = calculate_sale_time_statistics(series, window_size)\n",
    "        \n",
    "        if time_stats is None:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Check trimmed mean sale time\n",
    "        avg_time = time_stats['avg_interval_hours']\n",
    "        if avg_time > avg_trimmed_hours_max:\n",
    "            skipped_time_count += 1\n",
    "            continue\n",
    "        \n",
    "        # Check distribution quality and save rejected\n",
    "        if time_stats['coefficient_of_variation'] > max_cv:\n",
    "            skipped_cv_count += 1\n",
    "            rejected_series['cv'].append((series, time_stats))\n",
    "            continue\n",
    "        elif time_stats['range_ratio'] > max_range_ratio:\n",
    "            skipped_range_count += 1\n",
    "            rejected_series['range_ratio'].append((series, time_stats))\n",
    "            continue\n",
    "        elif time_stats['gap_ratio'] > max_gap_ratio:\n",
    "            skipped_gap_count += 1\n",
    "            rejected_series['gap_ratio'].append((series, time_stats))\n",
    "            continue\n",
    "        elif time_stats['outlier_ratio'] > max_outlier_ratio:\n",
    "            skipped_outlier_count += 1\n",
    "            rejected_series['outlier_ratio'].append((series, time_stats))\n",
    "            continue\n",
    "        \n",
    "        # Add statistics to metadata\n",
    "        series.metadata['sale_time_stats'] = time_stats\n",
    "        filtered_series.append(series)\n",
    "    \n",
    "    # Create new dataset\n",
    "    filtered_dataset = TimeSeriesDataset(\n",
    "        name=f\"{dataset.name} (Filtered by Sale Time)\",\n",
    "        description=f\"Dataset filtered by sale time. Created {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        series=filtered_series,\n",
    "        metadata={\n",
    "            **dataset.metadata,\n",
    "            'filter_by_sale_time': True,\n",
    "            'sale_time_window': window_size,\n",
    "            'avg_trimmed_hours_max': avg_trimmed_hours_max,\n",
    "            'max_cv': max_cv,\n",
    "            'max_range_ratio': max_range_ratio,\n",
    "            'max_gap_ratio': max_gap_ratio,\n",
    "            'max_outlier_ratio': max_outlier_ratio,\n",
    "            'original_count': original_count,\n",
    "            'filtered_count': len(filtered_series)\n",
    "        },\n",
    "        created_at=datetime.now()\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFiltering completed:\")\n",
    "    print(f\"  - Original series count: {original_count}\")\n",
    "    print(f\"  - Skipped (insufficient data): {skipped_count}\")\n",
    "    print(f\"  - Skipped (too long sale time): {skipped_time_count}\")\n",
    "    print(f\"  - Skipped (high variation): {skipped_cv_count}\")\n",
    "    print(f\"  - Skipped (large range): {skipped_range_count}\")\n",
    "    print(f\"  - Skipped (large gaps): {skipped_gap_count}\")\n",
    "    print(f\"  - Skipped (many outliers): {skipped_outlier_count}\")\n",
    "    print(f\"  - Remaining series: {len(filtered_dataset)}\")\n",
    "    print(f\"  - Retained percentage: {len(filtered_dataset)/original_count*100:.1f}%\")\n",
    "    \n",
    "    return filtered_dataset, rejected_series\n",
    "\n",
    "\n",
    "\n",
    "# Sale Time Filtering Settings\n",
    "# This dictionary defines parameters for the filter_dataset_by_sale_time function.\n",
    "# The filter analyzes time intervals between last sales (including to current date)\n",
    "# to select series with regular, stable sales. Core idea: calculate interval statistics\n",
    "# (after trimming outliers), check against thresholds, and reject series with rare,\n",
    "# unstable, or anomalous sales. Math includes min/max trimming, CV, ranges, etc.\n",
    "# for distribution quality detection. Rejected series are saved in rejected_series for analysis.\n",
    "\n",
    "\n",
    "# check below for more details (next cell)\n",
    "FILTER_SETTINGS = {\n",
    "\n",
    "\n",
    "    'window_size': 10,              # Number of last sales (points in time series) for time interval analysis.\n",
    "                                    # Math: Slice recent_points = series.points[-window_size:], sort by timestamp,\n",
    "                                    # add current date as virtual point, calculate Î”t_i = t_i - t_{i-1} (in seconds),\n",
    "                                    # total window_size intervals (including to current date).\n",
    "                                    # Core: Defines \"window\" for stability assessment; if points < window_size, series skipped (time_stats = None).\n",
    "                                    # Impact: Larger value provides reliable stats (longer history),\n",
    "                                    # but rejects short-history series; smaller allows more but stats may be noisy.\n",
    "                                    # Recommended: 5â€“20 for balance between data and accuracy.\n",
    "                                    # Example: For window_size=10 and 8 sales â€” skip; for 10 sales â€” 10 intervals for analysis.\n",
    "    \n",
    "    'avg_trimmed_hours_max': 50,    # Maximum trimmed mean time between sales (in hours) after outlier trimming.\n",
    "                                    # Math: trimmed_intervals = intervals without min/max (preserving current interval if min/max);\n",
    "                                    # intervals_hours = [Î”t_i / 3600]; avg = (âˆ‘ intervals_hours) / len; if avg > threshold, skip.\n",
    "                                    # Core: Selects series with frequent enough sales; trimming ignores anomalies, focusing on typical pace.\n",
    "                                    # Impact: Smaller value requires frequent sales (e.g., daily),\n",
    "                                    # larger allows rare (e.g., weekly), but risks including inactive items.\n",
    "                                    # Recommended: 24â€“168 hours depending on market (e.g., 50 for moderately active items).\n",
    "                                    # Example: trimmed_intervals = [10, 15, 20] h; avg â‰ˆ15 <50 â€” pass; avg=60 â€” skip (skipped_time_count +=1).\n",
    "    \n",
    "    'max_cv': 1,                  # Maximum coefficient of variation (CV) for intervals â€” measure of relative variability.\n",
    "                                    # Math: Î¼ = avg_interval_hours; ÏƒÂ² = (âˆ‘ (x_i - Î¼)Â²) / n; Ïƒ = âˆšÏƒÂ²; CV = Ïƒ / Î¼ (if Î¼>0, else âˆž);\n",
    "                                    # if CV > threshold, skip and save to rejected_series['cv'].\n",
    "                                    # Core: Detects interval stability â€” low CV means predictable sales, high â€” chaos (e.g., bursts).\n",
    "                                    # Impact: Low threshold (0.5â€“0.8) retains few but quality series; high (1.7+) allows more, including slightly unstable.\n",
    "                                    # Recommended: 0.5â€“1.0 for strict stability, up to 2.0 for flexibility.\n",
    "                                    # Example: Intervals [10,10,30]; Î¼â‰ˆ16.67, Ïƒâ‰ˆ9.43, CVâ‰ˆ0.57 <1.7 â€” pass; [1,1,200]; CV>1.7 â€” skip.\n",
    "    \n",
    "    'max_range_ratio': 1.5,         # Maximum range of intervals relative to mean ((max - min) / mean).\n",
    "                                    # Math: range = max(intervals_hours) - min; range_ratio = range / Î¼ (if Î¼>0, else âˆž);\n",
    "                                    # if > threshold, skip and save to rejected_series['range_ratio'].\n",
    "                                    # Core: Checks uniformity â€” large range indicates extreme pauses/bursts, even if CV is ok.\n",
    "                                    # Impact: Low threshold (1.5â€“2.5) requires uniformity, high (3.0+) tolerates differences.\n",
    "                                    # Recommended: 2.0â€“4.0 for balance between strict selection and inclusivity.\n",
    "                                    # Example: [5,10,15]; Î¼=10, range=10, ratio=1.0 <3.0 â€” pass; [1,1,100]; ratioâ‰ˆ4.85 >3.0 â€” skip.\n",
    "    \n",
    "    'max_gap_ratio': 3.0,           # Maximum gap between neighboring sorted intervals (max_gap / avg_gap).\n",
    "                                    # Math: sorted_intervals; gaps_i = sorted_{i+1} - sorted_i; avg_gap = (âˆ‘ gaps) / (n-1);\n",
    "                                    # max_gap = max(gaps); gap_ratio = max_gap / avg_gap (if avg_gap>0, else âˆž); if > threshold, skip to rejected_series['gap_ratio'].\n",
    "                                    # Core: Detects clustering (groups of close intervals with large gaps) â€” sign of uneven sales (e.g., \"bursts\" + pauses).\n",
    "                                    # Impact: Low threshold (3.0â€“5.0) avoids clusters, high allows seasonal data.\n",
    "                                    # Recommended: 4.0â€“6.0 for data with potential seasonality.\n",
    "                                    # Example: sorted [1,2,10]; gaps [1,8]; avg_gap=4.5, max_gap=8, ratioâ‰ˆ1.78 <5.0 â€” pass; [1,2,50]; ratio>5.0 â€” skip.\n",
    "    \n",
    "    'max_outlier_ratio': 0.3        # Maximum ratio of outliers in intervals (values > 2*Ïƒ from Î¼).\n",
    "                                    # Math: outliers = [x where |x - Î¼| > 2*Ïƒ]; outlier_ratio = len(outliers) / len(intervals_hours);\n",
    "                                    # if > threshold, skip and save to rejected_series['outlier_ratio'].\n",
    "                                    # Core: Checks data quality â€” many outliers mean anomalies (e.g., rare events), indicating instability.\n",
    "                                    # Impact: Low threshold (0.1â€“0.3) requires few anomalies, high allows noisy data.\n",
    "                                    # Recommended: 0.2â€“0.4 for balance between strictness and data volume.\n",
    "                                    # Example: Intervals [10,11,12,50]; Î¼â‰ˆ20.75, Ïƒâ‰ˆ17.7; outliers=[50] (ratio=0.25 <0.3) â€” pass; 2 outliers of 5 (ratio=0.4 >0.3) â€” skip.\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Applying filtering with settings:\")\n",
    "for key, value in FILTER_SETTINGS.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "\n",
    "filtered_dataset, rejected_series = filter_dataset_by_sale_time(filtered_dataset_by_duplicates, **FILTER_SETTINGS)     \n",
    "# Display analysis of rejected series     \n",
    "# Update main dataset\n",
    "dataset = filtered_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00e16e",
   "metadata": {},
   "source": [
    "## Complementary Analysis of FILTER_SETTINGS Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb6efa6",
   "metadata": {},
   "source": [
    "\n",
    "These four parameters (`max_cv`, `max_range_ratio`, `max_gap_ratio`, and `max_outlier_ratio`) in the `FILTER_SETTINGS` dictionary form a multi-faceted system for filtering time series data based on sale interval stability. They analyze the distribution of time intervals between sales (after trimming outliers) to detect instability, irregularity, or anomalies. By combining global measures (like overall variability) with local ones (like gaps or specific outliers), they provide a robust, layered filter that reduces false positives/negatives compared to using any single metric.\n",
    "\n",
    "The parameters complement each other by addressing different aspects of distribution quality:\n",
    "- **Global vs. Local Focus**: CV and range_ratio assess overall spread, while gap_ratio and outlier_ratio zoom in on specific patterns (e.g., clusters or anomalies).\n",
    "- **Relative vs. Absolute**: CV and range_ratio normalize by the mean (relative), making them scale-invariant, whereas gap_ratio examines differences in sorted data, and outlier_ratio counts explicit deviations.\n",
    "- **Holistic Coverage**: Together, they catch scenarios where one metric might pass unstable data (e.g., high CV might miss clustered data that gap_ratio catches).\n",
    "\n",
    "Below, I'll describe:\n",
    "1. **How They Complement Each Other**: Pairwise explanations of synergies.\n",
    "2. **Strengths and Weaknesses**: For each parameter, its strong sides, weak sides, and how others compensate.\n",
    "\n",
    "### 1. How the Parameters Complement Each Other\n",
    "\n",
    "These metrics are applied sequentially in the filter (after checking average interval), so a series must pass *all* to be retained. This creates a \"defense in depth\" where each catches issues the others might miss, ensuring only stable, uniform distributions pass.\n",
    "\n",
    "- **max_cv and max_range_ratio**: CV measures overall relative dispersion (how spread out intervals are around the mean), but it can be insensitive to extreme values if they're balanced. Range_ratio complements by focusing solely on the extrema (max - min relative to mean), catching cases with rare but large deviations (e.g., one long pause) that don't heavily skew CV. Conversely, range_ratio ignores the distribution in between, so CV fills this by quantifying the full variability.\n",
    "\n",
    "- **max_cv and max_gap_ratio**: CV detects general instability but might pass clustered data (e.g., many short intervals + a few long ones averaging out). Gap_ratio complements by scanning sorted intervals for large \"jumps,\" identifying non-uniformity like bimodal distributions (clusters of quick sales followed by gaps). CV provides the broad picture, while gap_ratio adds granularity for uneven spacing.\n",
    "\n",
    "- **max_cv and max_outlier_ratio**: CV incorporates outliers into its calculation (via standard deviation), but if outliers are few, CV might stay low. Outlier_ratio explicitly counts them (using a 2Ïƒ threshold), catching sparse anomalies that don't dominate CV. Outlier_ratio adds a direct \"quality check,\" while CV ensures the overall spread isn't too high even without outliers.\n",
    "\n",
    "- **max_range_ratio and max_gap_ratio**: Range_ratio spots large overall spreads but doesn't distinguish if the spread is due to even dispersion or clusters. Gap_ratio complements by detecting if the spread comes from big gaps in the sorted list, revealing clustering (e.g., tight groups separated by voids). Range_ratio sets a \"ceiling\" on extremes, and gap_ratio ensures the intervals are evenly distributed within that range.\n",
    "\n",
    "- **max_range_ratio and max_outlier_ratio**: Range_ratio can be inflated by a single outlier, but since it's calculated on trimmed data, it might understate issues if trimming removes too much. Outlier_ratio complements by quantifying how many values are anomalous relative to the mean and std dev, catching persistent outliers that range might normalize. Outlier_ratio provides a count-based view, while range_ratio focuses on magnitude.\n",
    "\n",
    "- **max_gap_ratio and max_outlier_ratio**: Gap_ratio identifies structural issues like clustering but might miss small, scattered outliers that don't create large gaps. Outlier_ratio complements by flagging individual extreme values (e.g., isolated long intervals not forming a \"gap\" in sorted order). Gap_ratio handles distributional shape, while outlier_ratio ensures low anomaly density.\n",
    "\n",
    "**Overall Synergy**: Using all four creates a comprehensive filter. For example, a series with moderate CV but one huge outlier might pass CV but fail outlier_ratio or range_ratio. Clustered data might pass range_ratio (if extremes aren't too far) but fail gap_ratio. This reduces over-filtering (retaining more data) while minimizing unstable series slipping through.\n",
    "\n",
    "### 2. Strengths and Weaknesses of Each Parameter\n",
    "\n",
    "For each parameter, I'll outline its strengths, weaknesses, and how the others compensate (filling gaps).\n",
    "\n",
    "#### max_cv (Coefficient of Variation)\n",
    "- **Strengths**:\n",
    "  - Captures relative variability (scale-invariant), making it robust for intervals of different magnitudes.\n",
    "  - Holistic: Considers all data points via mean and std dev, good for detecting general instability.\n",
    "  - Sensitive to both high and low variability (e.g., chaotic bursts).\n",
    "- **Weaknesses**:\n",
    "  - Can be skewed by the mean (e.g., if Î¼ is small, even minor variations inflate CV).\n",
    "  - Insensitive to distribution shape (e.g., passes clustered or multimodal data if overall spread is low).\n",
    "  - Averages out extremes if they're not dominant.\n",
    "- **How Others Compensate**:\n",
    "  - Range_ratio fills the extreme-focus gap by emphasizing max-min differences.\n",
    "  - Gap_ratio addresses shape insensitivity by detecting clusters.\n",
    "  - Outlier_ratio adds explicit anomaly counting, catching what CV averages over.\n",
    "\n",
    "#### max_range_ratio (Range Relative to Mean)\n",
    "- **Strengths**:\n",
    "  - Simple and direct: Focuses on extrema, catching rare but impactful deviations (e.g., long pauses).\n",
    "  - Relative to mean, so scale-invariant like CV.\n",
    "  - Effective after trimming, as it ignores absolute outliers and checks core spread.\n",
    "- **Weaknesses**:\n",
    "  - Ignores internal distribution (e.g., passes if extremes are far but middle is uniform).\n",
    "  - Sensitive to sample size (small n can exaggerate range).\n",
    "  - Doesn't distinguish between even spread and clustered extremes.\n",
    "- **How Others Compensate**:\n",
    "  - CV provides overall variability, filling the \"internal\" gap.\n",
    "  - Gap_ratio detects if the range is due to clustering/gaps.\n",
    "  - Outlier_ratio quantifies if the range is driven by multiple anomalies.\n",
    "\n",
    "#### max_gap_ratio (Max Gap Relative to Average Gap)\n",
    "- **Strengths**:\n",
    "  - Specifically detects non-uniformity and clustering in sorted data (e.g., seasonal bursts).\n",
    "  - Focuses on local differences, revealing patterns like bimodality that global metrics miss.\n",
    "  - Robust to overall scale, as it's relative to average gap.\n",
    "- **Weaknesses**:\n",
    "  - Less effective for uniformly high variability (no large gaps but consistent noise).\n",
    "  - Requires sorted data and enough points (small n might yield infinite ratios).\n",
    "  - Ignores the magnitude of intervals, focusing only on gaps between them.\n",
    "- **How Others Compensate**:\n",
    "  - CV covers uniform high variability.\n",
    "  - Range_ratio handles overall magnitude of spreads.\n",
    "  - Outlier_ratio catches isolated extremes that don't form detectable gaps.\n",
    "\n",
    "#### max_outlier_ratio (Outlier Ratio)\n",
    "- **Strengths**:\n",
    "  - Directly quantifies data quality by counting anomalies (using statistical threshold: >2Ïƒ).\n",
    "  - Flexible for noisy data; allows some outliers without rejecting everything.\n",
    "  - Independent of distribution shape, focusing on deviations from the mean.\n",
    "- **Weaknesses**:\n",
    "  - Depends on accurate mean/std dev (if data is skewed, \"outliers\" might be misidentified).\n",
    "  - Doesn't capture overall spread or patterns (e.g., passes if outliers are few but clustered).\n",
    "  - Threshold (2Ïƒ) is arbitrary; might over/under-count in non-normal distributions.\n",
    "- **How Others Compensate**:\n",
    "  - CV incorporates outliers into global variability.\n",
    "  - Range_ratio catches extreme magnitudes even if not counted as outliers.\n",
    "  - Gap_ratio detects if outliers create structural gaps/clusters.\n",
    "\n",
    "This setup ensures balanced filtering: strengths amplify coverage, while weaknesses are mitigated by complements. If your data has specific characteristics (e.g., heavy seasonality), adjust thresholds to emphasize certain metrics (e.g., higher `max_gap_ratio`). For further tuning, consider correlating these with domain knowledge or visualizing rejected series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5d918",
   "metadata": {},
   "source": [
    "## Visualization of accepted series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accepted_series_analysis(dataset, max_examples=10, window_size=10):\n",
    "    \"\"\"Show analysis of accepted series with individual plots and timestamps\"\"\"\n",
    "    print(f\"\\nAnalysis of accepted series (showing up to {max_examples} examples):\")\n",
    "    \n",
    "    if not dataset or len(dataset) == 0:\n",
    "        print(\"No accepted series for analysis\")\n",
    "        return\n",
    "    \n",
    "    # Sort series by quality (best first)\n",
    "    # Use average sale time as a quality indicator (lower is better)\n",
    "    series_with_stats = []\n",
    "    \n",
    "    for series in dataset.series:\n",
    "        # Get sale time statistics for each series\n",
    "        stats = series.metadata.get('sale_time_stats')\n",
    "        if stats:\n",
    "            series_with_stats.append((series, stats))\n",
    "    \n",
    "    # Sort by average sale time (best = lower values)\n",
    "    sorted_accepted = sorted(series_with_stats, key=lambda x: x[1]['avg_interval_hours'])\n",
    "    \n",
    "    # Show statistics for all accepted series\n",
    "    if series_with_stats:\n",
    "        avg_times = [stats['avg_interval_hours'] for _, stats in series_with_stats]\n",
    "        cvs = [stats['coefficient_of_variation'] for _, stats in series_with_stats]\n",
    "        current_intervals = [stats.get('current_interval_hours', 0) for _, stats in series_with_stats]\n",
    "        \n",
    "        print(f\"\\n=== Statistics for all accepted series ===\")\n",
    "        print(f\"Total accepted: {len(series_with_stats)}\")\n",
    "        print(f\"    - Average sale time: {sum(avg_times)/len(avg_times):.2f} hours\")\n",
    "        print(f\"    - Average CV: {sum(cvs)/len(cvs):.3f}\")\n",
    "        print(f\"    - Average interval to current date: {sum(current_intervals)/len(current_intervals):.2f} hours\")\n",
    "        print(f\"    - Best sale time: {min(avg_times):.2f} hours\")\n",
    "        print(f\"    - Worst sale time: {max(avg_times):.2f} hours\")\n",
    "    \n",
    "    # Show individual plots for the best examples\n",
    "    examples_to_show = min(max_examples, len(sorted_accepted))\n",
    "    \n",
    "    print(f\"\\n=== Showing {examples_to_show} best examples ===\")\n",
    "    \n",
    "    for i in range(examples_to_show):\n",
    "        series, stats = sorted_accepted[i]\n",
    "        values = series.get_values()\n",
    "        timestamps = series.get_timestamps()\n",
    "        \n",
    "        # Take only the last N points for the plot\n",
    "        if len(values) > window_size:\n",
    "            values = values[-window_size:]\n",
    "            timestamps = timestamps[-window_size:]\n",
    "        \n",
    "        # Add the current date as a virtual point\n",
    "        current_timestamp = datetime.now().timestamp()\n",
    "        current_datetime = datetime.now()\n",
    "        \n",
    "        # Add the current date to the plot data\n",
    "        extended_timestamps = timestamps + [current_timestamp]\n",
    "        # For the price at the current moment, use the last known price\n",
    "        extended_values = values + [values[-1]] # Last price as current\n",
    "        \n",
    "        # Create a separate plot for each series\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Convert timestamps to datetime for better display\n",
    "        if isinstance(extended_timestamps[0], (int, float)):\n",
    "            # If timestamps are in Unix format, convert to datetime\n",
    "            dates = [datetime.fromtimestamp(ts) for ts in extended_timestamps]\n",
    "        else:\n",
    "            dates = extended_timestamps\n",
    "        \n",
    "        # Print the difference in hours between consecutive dates\n",
    "        diffs_hours = []\n",
    "        for j in range(1, len(dates)):\n",
    "            delta = (dates[j] - dates[j-1]).total_seconds() / 3600\n",
    "            diffs_hours.append(round(delta, 2))\n",
    "        print(f\"Series {i+1} - Differences between dates (hours): {diffs_hours}\")\n",
    "\n",
    "        # Plot the time series (including the current date)\n",
    "        plt.plot(dates[:-1], values, 'g-o', markersize=4, linewidth=2, label='Historical sales')\n",
    "        \n",
    "        # Add the current date as a separate point\n",
    "        plt.plot(dates[-1], extended_values[-1], 'r*', markersize=8, label='Current date')\n",
    "        \n",
    "        # Connect the last sale to the current date with a dashed line\n",
    "        plt.plot([dates[-2], dates[-1]], [values[-1], extended_values[-1]], 'r--', alpha=0.7)\n",
    "        \n",
    "        # Axis setup\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Price')\n",
    "        \n",
    "        # Show the series quality\n",
    "        quality_score = f\"Quality: {stats['avg_interval_hours']:.1f}h (lower is better)\"\n",
    "        \n",
    "        plt.title(f'ACCEPTED SERIES: {series.name}\\n{quality_score}\\n(Showing last {len(values)} points + current date)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Rotate time labels for better readability\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add statistics information to the plot\n",
    "        current_interval_info = f\"â€¢ Interval to current date: {stats.get('current_interval_hours', 0):.2f} hours\"\n",
    "        if stats.get('is_current_interval_outlier', False):\n",
    "            current_interval_info += \" (outlier!)\"\n",
    "        \n",
    "        # Show all metrics with green color (accepted)\n",
    "        cv_text = f\"â€¢ Coefficient of variation: {stats['coefficient_of_variation']:.3f} âœ“\"\n",
    "        range_text = f\"â€¢ Range/mean: {stats['range_ratio']:.3f} âœ“\"\n",
    "        gap_text = f\"â€¢ Max gap: {stats['gap_ratio']:.3f} âœ“\"\n",
    "        outlier_text = f\"â€¢ Outlier ratio: {stats['outlier_ratio']:.3f} âœ“\"\n",
    "        \n",
    "        info_text = f\"\"\"Sale time statistics (last {window_size} sales + current date):\n",
    "â€¢ Average time: {stats['avg_interval_hours']:.2f} hours âœ“\n",
    "{cv_text}\n",
    "{range_text}\n",
    "{gap_text}\n",
    "{outlier_text}\n",
    "â€¢ Number of intervals: {stats['total_intervals']}\n",
    "{current_interval_info}\"\"\"\n",
    "        \n",
    "        plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, \n",
    "                 verticalalignment='top', fontsize=10,\n",
    "                 bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"    Plot {i+1}/{examples_to_show}: {series.name} - Average time: {stats['avg_interval_hours']:.2f}h\")\n",
    "\n",
    "# Usage:\n",
    "show_accepted_series_analysis(dataset, max_examples=10, window_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec688d1c",
   "metadata": {},
   "source": [
    "## Visualization of rejected series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8544f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rejected_series_analysis(rejected_series, max_examples=10, custom_rejected_type=None):\n",
    "    \"\"\"Show analysis of rejected series with individual plots and timestamps\"\"\"\n",
    "    print(f\"\\nAnalysis of rejected series (showing up to {max_examples} examples of each type):\")\n",
    "    \n",
    "    for rejection_type, rejected_list in rejected_series.items():\n",
    "        if not rejected_list:\n",
    "            continue\n",
    "\n",
    "        if custom_rejected_type and custom_rejected_type != rejection_type:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n=== Rejected by {rejection_type.upper()} ===\")\n",
    "        print(f\"Total rejected: {len(rejected_list)}\")\n",
    "        \n",
    "        # Show statistics for rejected series\n",
    "        if rejected_list:\n",
    "            avg_times = [stats['avg_interval_hours'] for _, stats in rejected_list]\n",
    "            cvs = [stats['coefficient_of_variation'] for _, stats in rejected_list]\n",
    "            current_intervals = [stats.get('current_interval_hours', 0) for _, stats in rejected_list]\n",
    "            print(f\"    - Average sale time: {sum(avg_times)/len(avg_times):.2f} hours\")\n",
    "            print(f\"    - Average CV: {sum(cvs)/len(cvs):.3f}\")\n",
    "            print(f\"    - Average interval to current date: {sum(current_intervals)/len(current_intervals):.2f} hours\")\n",
    "        \n",
    "        # Sort by proximity to the threshold for each rejection type\n",
    "        if rejection_type == 'cv':\n",
    "            # Sort by ascending CV (closest to the threshold)\n",
    "            sorted_rejected = sorted(rejected_list, key=lambda x: x[1]['coefficient_of_variation'])\n",
    "        elif rejection_type == 'range_ratio':\n",
    "            # Sort by ascending range_ratio\n",
    "            sorted_rejected = sorted(rejected_list, key=lambda x: x[1]['range_ratio'])\n",
    "        elif rejection_type == 'gap_ratio':\n",
    "            # Sort by ascending gap_ratio\n",
    "            sorted_rejected = sorted(rejected_list, key=lambda x: x[1]['gap_ratio'])\n",
    "        elif rejection_type == 'outlier_ratio':\n",
    "            # Sort by ascending outlier_ratio\n",
    "            sorted_rejected = sorted(rejected_list, key=lambda x: x[1]['outlier_ratio'])\n",
    "        else:\n",
    "            sorted_rejected = rejected_list\n",
    "        \n",
    "        # Show individual plots for each example (starting with the most borderline cases)\n",
    "        examples_to_show = min(max_examples, len(sorted_rejected))\n",
    "        \n",
    "        print(f\"    Showing {examples_to_show} most borderline cases:\")\n",
    "        \n",
    "        for i in range(examples_to_show):\n",
    "            series, stats = sorted_rejected[i]\n",
    "            values = series.get_values()\n",
    "            timestamps = series.get_timestamps()\n",
    "            \n",
    "            # Take only the last 10 points for the plot\n",
    "            window_size = 10\n",
    "            if len(values) > window_size:\n",
    "                values = values[-window_size:]\n",
    "                timestamps = timestamps[-window_size:]\n",
    "            \n",
    "            # Add the current date as a virtual point\n",
    "            current_timestamp = datetime.now().timestamp()\n",
    "            current_datetime = datetime.now()\n",
    "            \n",
    "            # Add the current date to the plot data\n",
    "            extended_timestamps = timestamps + [current_timestamp]\n",
    "            # For the price at the current moment, use the last known price\n",
    "            extended_values = values + [values[-1]] # Last price as current\n",
    "            \n",
    "            # Create a separate plot for each series\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Convert timestamps to datetime for better display\n",
    "            if isinstance(extended_timestamps[0], (int, float)):\n",
    "                # If timestamps are in Unix format, convert to datetime\n",
    "                dates = [datetime.fromtimestamp(ts) for ts in extended_timestamps]\n",
    "            else:\n",
    "                dates = extended_timestamps\n",
    "            \n",
    "            # Print the difference in hours between consecutive dates\n",
    "            diffs_hours = []\n",
    "            for j in range(1, len(dates)):\n",
    "                delta = (dates[j] - dates[j-1]).total_seconds() / 3600\n",
    "                diffs_hours.append(round(delta, 2))\n",
    "            print(\"Differences between dates (hours):\", diffs_hours)\n",
    "\n",
    "            # Plot the time series (including the current date)\n",
    "            plt.plot(dates[:-1], values, 'b-o', markersize=4, linewidth=2, label='Historical sales')\n",
    "            \n",
    "            # Add the current date as a separate point\n",
    "            plt.plot(dates[-1], extended_values[-1], 'r*', markersize=8, label='Current date')\n",
    "            \n",
    "            # Connect the last sale to the current date with a dashed line\n",
    "            plt.plot([dates[-2], dates[-1]], [values[-1], extended_values[-1]], 'r--', alpha=0.7)\n",
    "            \n",
    "            # Axis setup\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Price')\n",
    "            \n",
    "            # Show the value that led to rejection\n",
    "            threshold_value = \"\"\n",
    "            if rejection_type == 'cv':\n",
    "                threshold_value = f\"CV: {stats['coefficient_of_variation']:.3f} (threshold: {FILTER_SETTINGS['max_cv']})\"\n",
    "            elif rejection_type == 'range_ratio':\n",
    "                threshold_value = f\"Range: {stats['range_ratio']:.3f} (threshold: {FILTER_SETTINGS['max_range_ratio']})\"\n",
    "            elif rejection_type == 'gap_ratio':\n",
    "                threshold_value = f\"Gap: {stats['gap_ratio']:.3f} (threshold: {FILTER_SETTINGS['max_gap_ratio']})\"\n",
    "            elif rejection_type == 'outlier_ratio':\n",
    "                threshold_value = f\"Outliers: {stats['outlier_ratio']:.3f} (threshold: {FILTER_SETTINGS['max_outlier_ratio']})\"\n",
    "            \n",
    "            plt.title(f'Series: {series.name}\\nRejected by: {rejection_type.upper()}\\n{threshold_value}\\nAvg: {stats[\"avg_interval_hours\"]:.1f}h\\n(Showing last {len(values)} points + current date)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Rotate time labels for better readability\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "            # Add statistics information to the plot\n",
    "            current_interval_info = f\"â€¢ Interval to current date: {stats.get('current_interval_hours', 0):.2f} hours\"\n",
    "            if stats.get('is_current_interval_outlier', False):\n",
    "                current_interval_info += \" (outlier!)\"\n",
    "            \n",
    "            # Show all metrics, highlighting the one that caused the rejection\n",
    "            cv_text = f\"â€¢ Coefficient of variation: {stats['coefficient_of_variation']:.3f}\"\n",
    "            range_text = f\"â€¢ Range/mean: {stats['range_ratio']:.3f}\"\n",
    "            gap_text = f\"â€¢ Max gap: {stats['gap_ratio']:.3f}\"\n",
    "            outlier_text = f\"â€¢ Outlier ratio: {stats['outlier_ratio']:.3f}\"\n",
    "            \n",
    "            # Highlight the problematic metric\n",
    "            if rejection_type == 'cv':\n",
    "                cv_text += \" â† PROBLEM\"\n",
    "            elif rejection_type == 'range_ratio':\n",
    "                range_text += \" â† PROBLEM\"\n",
    "            elif rejection_type == 'gap_ratio':\n",
    "                gap_text += \" â† PROBLEM\"\n",
    "            elif rejection_type == 'outlier_ratio':\n",
    "                outlier_text += \" â† PROBLEM\"\n",
    "            \n",
    "            info_text = f\"\"\"Sale time statistics (last {window_size} sales + current date):\n",
    "â€¢ Average time: {stats['avg_interval_hours']:.2f} hours - {FILTER_SETTINGS['avg_trimmed_hours_max']}\n",
    "{cv_text} - {FILTER_SETTINGS['max_cv']}\n",
    "{range_text} - {FILTER_SETTINGS['max_range_ratio']}\n",
    "{gap_text} - {FILTER_SETTINGS['max_gap_ratio']}\n",
    "{outlier_text} - {FILTER_SETTINGS['max_outlier_ratio']}\n",
    "â€¢ Number of intervals: {stats['total_intervals']}\n",
    "{current_interval_info}\"\"\"\n",
    "            \n",
    "            plt.text(0.02, 0.98, info_text, transform=plt.gca().transAxes, \n",
    "                     verticalalignment='top', fontsize=10,\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"    Plot {i+1}/{examples_to_show}: {series.name} - {threshold_value}\")\n",
    "\n",
    "show_rejected_series_analysis(rejected_series, max_examples=10, custom_rejected_type='range_ratio')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
